{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yusmitha-Lekha/YusmithaLekha_INFO5731_Fall2024/blob/main/Prathi_YusmithaLekha_Exercise_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBKvD6O_TY6e"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OjkjVZzNtNI"
      },
      "source": [
        "# write your answer here\n",
        "**Reasearch question:**\"What are the key challenges faced by international students in higher education, and how do these challenges affect their academic performance and overall well-being?\"\n",
        "\n",
        "**kind of data should be collected**:Name, ID, Age, Field of Study, GPA, Attendance, Country of Origin, Host Country, Language Barriers, Cultural Differences, Differences in Educational Systems.\n",
        "\n",
        "**Amount of data needed for analysis**:In order to make insightful decisions, you should gather information from 500 to 1,000 international students studying in various fields and nations.Responses from students who have studied abroad for varying periods of time (e.g., one year, two years, or more) should be included in the data.\n",
        "\n",
        "**Detailed steps for collecting and saving the data.**:\n",
        "\n",
        "\n",
        "1.   Create a survey that includes demographic information (Name, ID, Age, Study Subject, Country of Origin, Host Country) as well as important details like Attendance, GPA, Language Barriers, Cultural Differences, and Educational System Differences.\n",
        "2.   To reach overseas students, distribute the survey using internet resources (SurveyMonkey, Google Forms), and work with universities to coordinate distribution.\n",
        "3.   Utilizing programs like Python or Excel, gather, purify, and evaluate the data; afterwards, safely store the dataset for future study..\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9RqrlwdTfvl"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XvRknixTh1g",
        "outputId": "518b2457-1d76-46e5-8e71-e2d223983f77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Name    ID  Age Field of Study Country of Origin Host Country  \\\n",
            "0  Student_1  ID_1   21           STEM            Canada    Australia   \n",
            "1  Student_2  ID_2   20            Law           Nigeria          USA   \n",
            "2  Student_3  ID_3   18            Law             China       Canada   \n",
            "3  Student_4  ID_4   23       Medicine           Nigeria    Australia   \n",
            "4  Student_5  ID_5   21           STEM           Nigeria          USA   \n",
            "\n",
            "   Language Barriers  Cultural Differences  Educational System Differences  \\\n",
            "0                  2                     2                               2   \n",
            "1                  1                     1                               2   \n",
            "2                  4                     2                               4   \n",
            "3                  3                     2                               2   \n",
            "4                  3                     3                               5   \n",
            "\n",
            "    GPA  Attendance  \n",
            "0  3.47          94  \n",
            "1  2.47          98  \n",
            "2  3.18          60  \n",
            "3  3.91          81  \n",
            "4  2.53          62  \n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "flds_of_stdy = ['STEM', 'Humanities', 'Business', 'Arts', 'Law', 'Medicine']\n",
        "cntries_of_orgn = ['India', 'China', 'Germany', 'Nigeria', 'Brazil', 'Canada']\n",
        "hst_cntris = ['USA', 'UK', 'Australia', 'Germany', 'Canada']\n",
        "\n",
        "def gnrate_ch():\n",
        "    return random.randint(1, 5)\n",
        "\n",
        "dt = []\n",
        "for i in range(1, 1001):\n",
        "    sample = {\n",
        "        'Name': f'Student_{i}',\n",
        "        'ID': f'ID_{i}',\n",
        "        'Age': random.randint(18, 35),\n",
        "        'Field of Study': random.choice(flds_of_stdy),\n",
        "        'Country of Origin': random.choice(cntries_of_orgn),\n",
        "        'Host Country': random.choice(hst_cntris),\n",
        "        'Language Barriers': gnrate_ch(),\n",
        "        'Cultural Differences': gnrate_ch(),\n",
        "        'Educational System Differences': gnrate_ch(),\n",
        "        'GPA': round(random.uniform(2.0, 4.0), 2),\n",
        "        'Attendance': random.randint(60, 100)\n",
        "    }\n",
        "    dt.append(sample)\n",
        "\n",
        "df = pd.DataFrame(dt)\n",
        "\n",
        "df.to_csv('Internationaal_Students_Challenges.csv', index=False)\n",
        "\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0FojuxrUfEV",
        "outputId": "cb640eff-2781-4597-b6af-9b503d6d95f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scholarly\n",
            "  Downloading scholarly-1.7.11-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting arrow (from scholarly)\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from scholarly) (4.12.3)\n",
            "Collecting bibtexparser (from scholarly)\n",
            "  Downloading bibtexparser-1.4.1.tar.gz (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting deprecated (from scholarly)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting fake-useragent (from scholarly)\n",
            "  Downloading fake_useragent-1.5.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting free-proxy (from scholarly)\n",
            "  Downloading free_proxy-1.1.2.tar.gz (5.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx (from scholarly)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from scholarly) (1.0.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from scholarly) (2.32.3)\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.10/dist-packages (from scholarly) (4.24.0)\n",
            "Collecting sphinx-rtd-theme (from scholarly)\n",
            "  Downloading sphinx_rtd_theme-2.0.0-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from scholarly) (4.12.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from arrow->scholarly) (2.8.2)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow->scholarly)\n",
            "  Downloading types_python_dateutil-2.9.0.20240906-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->scholarly) (2.6)\n",
            "Requirement already satisfied: pyparsing>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from bibtexparser->scholarly) (3.1.4)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated->scholarly) (1.16.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from free-proxy->scholarly) (4.9.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->scholarly) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->scholarly) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx->scholarly)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->scholarly) (3.8)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->scholarly) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->scholarly) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->scholarly) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->scholarly) (1.26.16)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->scholarly) (1.7.1)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.10/dist-packages (from selenium->scholarly) (0.26.2)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.10/dist-packages (from selenium->scholarly) (0.11.1)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium->scholarly) (1.8.0)\n",
            "Requirement already satisfied: sphinx<8,>=5 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme->scholarly) (5.0.2)\n",
            "Requirement already satisfied: docutils<0.21 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme->scholarly) (0.18.1)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->scholarly)\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7.0->arrow->scholarly) (1.16.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (3.1.4)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.16.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.16.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (0.7.16)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (1.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (24.1)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->scholarly) (24.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->scholarly) (2.4.0)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->scholarly) (1.3.0.post0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->scholarly) (1.2.2)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.10/dist-packages (from trio-websocket~=0.9->selenium->scholarly) (1.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.3->sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.1.5)\n",
            "Downloading scholarly-1.7.11-py3-none-any.whl (39 kB)\n",
            "Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading fake_useragent-1.5.1-py3-none-any.whl (17 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinx_rtd_theme-2.0.0-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_python_dateutil-2.9.0.20240906-py3-none-any.whl (9.7 kB)\n",
            "Building wheels for collected packages: bibtexparser, free-proxy\n",
            "  Building wheel for bibtexparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bibtexparser: filename=bibtexparser-1.4.1-py3-none-any.whl size=43253 sha256=e08caa2fbc825492b211ffb887933efa553f8695c22a47fcad22d8909a5584d8\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/c6/c3/56e639fab68d1fdbf13ea147636d9795ccdbd3c1d3178d1332\n",
            "  Building wheel for free-proxy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for free-proxy: filename=free_proxy-1.1.2-py3-none-any.whl size=5813 sha256=f3cd343c2576dfcb3853d01d3672b75556d5151a7c4e55b0485d14e8666034d0\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/db/d5/089b3eb5f67a2103a5136b9043156af7fb96fac4cae00fe006\n",
            "Successfully built bibtexparser free-proxy\n",
            "Installing collected packages: fake-useragent, types-python-dateutil, httpcore, deprecated, bibtexparser, httpx, free-proxy, arrow, sphinxcontrib-jquery, sphinx-rtd-theme, scholarly\n",
            "Successfully installed arrow-1.3.0 bibtexparser-1.4.1 deprecated-1.2.14 fake-useragent-1.5.1 free-proxy-1.1.2 httpcore-1.0.5 httpx-0.27.2 scholarly-1.7.11 sphinx-rtd-theme-2.0.0 sphinxcontrib-jquery-4.1 types-python-dateutil-2.9.0.20240906\n"
          ]
        }
      ],
      "source": [
        "!pip install scholarly\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scholarly import scholarly\n",
        "import pandas as pd\n",
        "import csv\n",
        "import time\n",
        "\n",
        "query = \"XYZ\"\n",
        "file_name = 'Glg_scholar_articles.csv'\n",
        "batch_size = 100\n",
        "total_articles = 1000\n",
        "\n",
        "def extract_article_info(article):\n",
        "    title = article.get('bib', {}).get('title', 'No Title')\n",
        "    venue = article.get('bib', {}).get('venue', 'No Venue')\n",
        "    year = article.get('bib', {}).get('pub_year', 'No Year')\n",
        "    authors = ', '.join(article.get('bib', {}).get('author', []))\n",
        "    abstract = article.get('bib', {}).get('abstract', 'No Abstract Available')\n",
        "\n",
        "    return {\n",
        "        'Title': title,\n",
        "        'Venue': venue,\n",
        "        'Year': year,\n",
        "        'Authors': authors,\n",
        "        'Abstract': abstract\n",
        "    }\n",
        "\n",
        "with open(file_name, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.DictWriter(file, fieldnames=['Title', 'Venue', 'Year', 'Authors', 'Abstract'])\n",
        "    writer.writeheader()\n",
        "\n",
        "    collected_articles = 0\n",
        "    page = 1\n",
        "\n",
        "    while collected_articles < total_articles:\n",
        "        try:\n",
        "            search_query = scholarly.search_pubs(query)\n",
        "            articles_to_save = []\n",
        "\n",
        "            for i, article in enumerate(search_query):\n",
        "                if collected_articles >= total_articles:\n",
        "                    break\n",
        "\n",
        "                article_info = extract_article_info(article)\n",
        "                articles_to_save.append(article_info)\n",
        "                collected_articles += 1\n",
        "\n",
        "                if len(articles_to_save) >= batch_size:\n",
        "                    break\n",
        "\n",
        "            writer.writerows(articles_to_save)\n",
        "\n",
        "            print(f\"Saved {collected_articles} articles so far...\")\n",
        "\n",
        "            time.sleep(5)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error after {collected_articles} articles: {e}\")\n",
        "            break\n",
        "\n",
        "print(f\"Data collection complete. A total of {collected_articles} articles saved to {file_name}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jcwb9V9f8nv5",
        "outputId": "01521231-7365-4298-c302-2f6769e77ac3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 100 articles so far...\n",
            "Saved 200 articles so far...\n",
            "Saved 300 articles so far...\n",
            "Saved 400 articles so far...\n",
            "Saved 500 articles so far...\n",
            "Saved 600 articles so far...\n",
            "Saved 700 articles so far...\n",
            "Saved 800 articles so far...\n",
            "Saved 900 articles so far...\n",
            "Error after 970 articles: Cannot Fetch from Google Scholar.\n",
            "Data collection complete. A total of 970 articles saved to Glg_scholar_articles.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qb_qi0d3ZYLs",
        "outputId": "dca4a5d0-48a5-42ca-9b06-0df9f3678954"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting praw\n",
            "  Downloading praw-7.7.1-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting prawcore<3,>=2.1 (from praw)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting update-checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from prawcore<3,>=2.1->praw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2024.8.30)\n",
            "Downloading praw-7.7.1-py3-none-any.whl (191 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.0/191.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Installing collected packages: update-checker, prawcore, praw\n",
            "Successfully installed praw-7.7.1 prawcore-2.4.0 update-checker-0.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install praw\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WremOoMSs5VW",
        "outputId": "1ecc69ce-e306-4fa0-d829-c3b2e4807729"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               Title  Score     Author  \\\n",
            "0                                       Web Scraping     37  G_S_7_wiz   \n",
            "1  Currency converter - my first web scraping pro...   2027   dimakiss   \n",
            "2                      Web Scraping 1010 with Python    954    sbskell   \n",
            "3  I taught myself web scraping today! Made an ap...   1097     oisack   \n",
            "4  Web Scraping with Python: Everything you need ...    846    japaget   \n",
            "\n",
            "   Comments  Upvote Ratio       Created  \\\n",
            "0        40          0.81  1.694761e+09   \n",
            "1       118          0.98  1.591800e+09   \n",
            "2       100          0.97  1.598971e+09   \n",
            "3        91          0.96  1.592189e+09   \n",
            "4        43          0.97  1.652634e+09   \n",
            "\n",
            "                                                 URL  \n",
            "0  https://www.reddit.com/r/Python/comments/16j68...  \n",
            "1                    https://i.imgur.com/VPPvQig.png  \n",
            "2  https://www.scrapingbee.com/blog/web-scraping-...  \n",
            "3                https://i.redd.it/3r2tv8pvkz451.png  \n",
            "4  https://www.scrapingbee.com/blog/web-scraping-...  \n"
          ]
        }
      ],
      "source": [
        "import praw\n",
        "import pandas as pd\n",
        "\n",
        "red_it = praw.Reddit(\n",
        "    client_id='e-mniU2xZD61-fY85ThqhA',\n",
        "    client_secret='noPwBG8B9ywRHR9B2MURBDarkf6uFg',\n",
        "    user_agent='Yusmitha Lekha',\n",
        ")\n",
        "\n",
        "subreddit = red_it.subreddit('Python')\n",
        "\n",
        "query = 'web scraping'\n",
        "\n",
        "posts = subreddit.search(query, limit=100)\n",
        "\n",
        "pst_data = []\n",
        "for post in posts:\n",
        "    post_info = {\n",
        "        'Title': post.title,\n",
        "        'Score': post.score,\n",
        "        'Author': str(post.author),\n",
        "        'Comments': post.num_comments,\n",
        "        'Upvote Ratio': post.upvote_ratio,\n",
        "        'Created': post.created_utc,\n",
        "        'URL': post.url\n",
        "    }\n",
        "    pst_data.append(post_info)\n",
        "\n",
        "df = pd.DataFrame(pst_data)\n",
        "\n",
        "df.to_csv('Reddit_Posts.csv', index=False)\n",
        "\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I57NXsauCec2"
      },
      "outputs": [],
      "source": [
        "# write your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZOhks1dXWEe"
      },
      "source": [
        "# Mandatory Question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqmHVEwaWhbV"
      },
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "akAVJn9YBTQT",
        "outputId": "fa9f9715-812c-441c-e0a3-d7b2d5d2f55a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThe web scratching and information assortment practices were a significant opportunity for growth. I acquired down to earth information about key procedures like utilizing APIs, dealing with rate restricts, and overseeing pagination to assemble huge datasets productively. Understanding how to extricate information from organized sources like APIs and how to defeat difficulties, for example, rate-restricting mistakes, extended my specialized abilities. The most difficult perspective was managing rate cutoff points and IP obstructing, which I settled by utilizing APIs and adding delays between demands. This ability is profoundly pertinent to my field, as information assortment from online sources upgrades research by giving admittance to certifiable information.\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "'''\n",
        "The web scratching and information assortment practices were a significant opportunity for growth. I acquired down to earth information about key procedures like utilizing APIs, dealing with rate restricts, and overseeing pagination to assemble huge datasets productively. Understanding how to extricate information from organized sources like APIs and how to defeat difficulties, for example, rate-restricting mistakes, extended my specialized abilities. The most difficult perspective was managing rate cutoff points and IP obstructing, which I settled by utilizing APIs and adding delays between demands. This ability is profoundly pertinent to my field, as information assortment from online sources upgrades research by giving admittance to certifiable information.\n",
        "\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "FBKvD6O_TY6e",
        "55W9AMdXCSpV"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}